{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# section 2\n"
      ],
      "metadata": {
        "id": "k0zv1gieUwBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kSkP07mvUzD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "pT1YSlDwUzh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIrnJZuwVS4_",
        "outputId": "771fbd34-081a-49e9-aecc-c2974e37d82a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> l\n",
            "\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] bcp47............... BCP-47 Language Tags\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "Hit Enter to continue: \n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "Hit Enter to continue: q\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\"Lorem Ipsum\" is a standard dummy text used in the printing and typesetting industry.\n",
        "# Sample Text for NLP Practice\n",
        "sample_text = \"\"\"\n",
        "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum eget felis a arcu eleifend ullamcorper.\n",
        "Quisque id pretium purus, eu auctor urna. Proin nec convallis libero. Integer efficitur odio ac ipsum sollicitudin, nec malesuada purus blandit.\n",
        "Sed a fringilla ligula. Nunc vel tristique erat. Sed euismod erat a justo convallis tristique.\n",
        "Praesent euismod odio euismod, commodo erat eu, eleifend nulla. Etiam vitae magna sit amet libero malesuada varius.\n",
        "Duis fermentum risus ut nisl venenatis, in iaculis est gravida. Aliquam at lacus hendrerit, lacinia ex non, facilisis odio.\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "aUwTWQ-GntCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the punkt tokenizer data\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_C8oLtwYKT2",
        "outputId": "bdcc74fe-9da0-4fa6-c6d8-891fa057867e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence tokenizing\n",
        "sentences = nltk.sent_tokenize(sample_text)\n",
        "sentences\n",
        "# return a list from sentences of sample _text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNStnTkoXd-_",
        "outputId": "2dafa2d8-1b2f-4da5-9cb3-820702c118e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\nLorem ipsum dolor sit amet, consectetur adipiscing elit.',\n",
              " 'Vestibulum eget felis a arcu eleifend ullamcorper.',\n",
              " 'Quisque id pretium purus, eu auctor urna.',\n",
              " 'Proin nec convallis libero.',\n",
              " 'Integer efficitur odio ac ipsum sollicitudin, nec malesuada purus blandit.',\n",
              " 'Sed a fringilla ligula.',\n",
              " 'Nunc vel tristique erat.',\n",
              " 'Sed euismod erat a justo convallis tristique.',\n",
              " 'Praesent euismod odio euismod, commodo erat eu, eleifend nulla.',\n",
              " 'Etiam vitae magna sit amet libero malesuada varius.',\n",
              " 'Duis fermentum risus ut nisl venenatis, in iaculis est gravida.',\n",
              " 'Aliquam at lacus hendrerit, lacinia ex non, facilisis odio.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word tokenizing\n",
        "words = nltk.word_tokenize(sample_text)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0p8Hxq9nYqVl",
        "outputId": "773257ee-dda0-4db2-8e8f-ea72759c29ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Lorem',\n",
              " 'ipsum',\n",
              " 'dolor',\n",
              " 'sit',\n",
              " 'amet',\n",
              " ',',\n",
              " 'consectetur',\n",
              " 'adipiscing',\n",
              " 'elit',\n",
              " '.',\n",
              " 'Vestibulum',\n",
              " 'eget',\n",
              " 'felis',\n",
              " 'a',\n",
              " 'arcu',\n",
              " 'eleifend',\n",
              " 'ullamcorper',\n",
              " '.',\n",
              " 'Quisque',\n",
              " 'id',\n",
              " 'pretium',\n",
              " 'purus',\n",
              " ',',\n",
              " 'eu',\n",
              " 'auctor',\n",
              " 'urna',\n",
              " '.',\n",
              " 'Proin',\n",
              " 'nec',\n",
              " 'convallis',\n",
              " 'libero',\n",
              " '.',\n",
              " 'Integer',\n",
              " 'efficitur',\n",
              " 'odio',\n",
              " 'ac',\n",
              " 'ipsum',\n",
              " 'sollicitudin',\n",
              " ',',\n",
              " 'nec',\n",
              " 'malesuada',\n",
              " 'purus',\n",
              " 'blandit',\n",
              " '.',\n",
              " 'Sed',\n",
              " 'a',\n",
              " 'fringilla',\n",
              " 'ligula',\n",
              " '.',\n",
              " 'Nunc',\n",
              " 'vel',\n",
              " 'tristique',\n",
              " 'erat',\n",
              " '.',\n",
              " 'Sed',\n",
              " 'euismod',\n",
              " 'erat',\n",
              " 'a',\n",
              " 'justo',\n",
              " 'convallis',\n",
              " 'tristique',\n",
              " '.',\n",
              " 'Praesent',\n",
              " 'euismod',\n",
              " 'odio',\n",
              " 'euismod',\n",
              " ',',\n",
              " 'commodo',\n",
              " 'erat',\n",
              " 'eu',\n",
              " ',',\n",
              " 'eleifend',\n",
              " 'nulla',\n",
              " '.',\n",
              " 'Etiam',\n",
              " 'vitae',\n",
              " 'magna',\n",
              " 'sit',\n",
              " 'amet',\n",
              " 'libero',\n",
              " 'malesuada',\n",
              " 'varius',\n",
              " '.',\n",
              " 'Duis',\n",
              " 'fermentum',\n",
              " 'risus',\n",
              " 'ut',\n",
              " 'nisl',\n",
              " 'venenatis',\n",
              " ',',\n",
              " 'in',\n",
              " 'iaculis',\n",
              " 'est',\n",
              " 'gravida',\n",
              " '.',\n",
              " 'Aliquam',\n",
              " 'at',\n",
              " 'lacus',\n",
              " 'hendrerit',\n",
              " ',',\n",
              " 'lacinia',\n",
              " 'ex',\n",
              " 'non',\n",
              " ',',\n",
              " 'facilisis',\n",
              " 'odio',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# persian text"
      ],
      "metadata": {
        "id": "-4G4uFzuank9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install hazm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 932
        },
        "id": "p82_wjY2Y8UE",
        "outputId": "e0d8f907-b27c-4ebe-d5e2-2a2eca3792b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hazm\n",
            "  Downloading hazm-0.9.4-py3-none-any.whl (371 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m371.7/371.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasttext-wheel<0.10.0,>=0.9.2 (from hazm)\n",
            "  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flashtext<3.0,>=2.7 (from hazm)\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (4.3.2)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.8.1)\n",
            "Collecting numpy==1.24.3 (from hazm)\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-crfsuite<0.10.0,>=0.9.9 (from hazm)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.2.2)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel<0.10.0,>=0.9.2->hazm)\n",
            "  Downloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (67.7.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (6.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.66.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.2.0)\n",
            "Building wheels for collected packages: flashtext\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9297 sha256=0488a8a84fb9acc7f20af7ee57e7a30b2da708f312065d845edbe37b82f66ecf\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/be/39/c37ad168eb2ff644c9685f52554440372129450f0b8ed203dd\n",
            "Successfully built flashtext\n",
            "Installing collected packages: python-crfsuite, flashtext, pybind11, numpy, fasttext-wheel, hazm\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fasttext-wheel-0.9.2 flashtext-2.7 hazm-0.9.4 numpy-1.24.3 pybind11-2.11.1 python-crfsuite-0.9.9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import *"
      ],
      "metadata": {
        "id": "AEKSilLwY8gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_persian = \"\"\"\n",
        "درختان در پارک زیبا هستند. آسمان آبی و صاف است. مردم در حال دیدن مناظر طبیعی هستند و لبخند می‌زنند. هوا بهاری است و هوای تازه‌ای داریم.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "Pz73JEjOY8jQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=nltk.sent_tokenize(sample_persian)\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgxBQUz8bgQx",
        "outputId": "56b85e67-8ec5-4b9c-d228-6bb3dcfc6361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\nدرختان در پارک زیبا هستند.',\n",
              " 'آسمان آبی و صاف است.',\n",
              " 'مردم در حال دیدن مناظر طبیعی هستند و لبخند می\\u200cزنند.',\n",
              " 'هوا بهاری است و هوای تازه\\u200cای داریم.']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words=nltk.word_tokenize(sample_persian)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cj_yryh1bpJH",
        "outputId": "e5e773a6-1ac7-47f2-dcc8-b2b0ac4e4f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['درختان',\n",
              " 'در',\n",
              " 'پارک',\n",
              " 'زیبا',\n",
              " 'هستند',\n",
              " '.',\n",
              " 'آسمان',\n",
              " 'آبی',\n",
              " 'و',\n",
              " 'صاف',\n",
              " 'است',\n",
              " '.',\n",
              " 'مردم',\n",
              " 'در',\n",
              " 'حال',\n",
              " 'دیدن',\n",
              " 'مناظر',\n",
              " 'طبیعی',\n",
              " 'هستند',\n",
              " 'و',\n",
              " 'لبخند',\n",
              " 'می\\u200cزنند',\n",
              " '.',\n",
              " 'هوا',\n",
              " 'بهاری',\n",
              " 'است',\n",
              " 'و',\n",
              " 'هوای',\n",
              " 'تازه\\u200cای',\n",
              " 'داریم',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the letter in persian language are recognize with unicode.\n",
        "\n",
        "Unicode is a standardized encoding system that assigns a unique numeric code to each character in the script. Unicode is designed to represent text characters from various languages and scripts, including Persian (Farsi).\n",
        "\n",
        "some common Persian characters and their corresponding Unicode code points:\n",
        "\n",
        "Persian Letter \"الف\" (Alef):\n",
        "\n",
        "Unicode Code Point: U+0627\n",
        "\n",
        "Persian Letter \"ب\" (Be):\n",
        "\n",
        "Unicode Code Point: U+0628\n",
        "\n",
        "Persian Letter \"پ\" (Pe):\n",
        "\n",
        "Unicode Code Point: U+067E\n",
        "\n",
        "Persian Letter \"ت\" (Te):\n",
        "\n",
        "Unicode Code Point: U+062A\n",
        "\n",
        "Persian Letter \"ث\" (Se):\n",
        "\n",
        "Unicode Code Point: U+062B\n",
        "\n",
        "\n",
        "arabic unicode is defferent from persian but the alphabet is the same.\n",
        "\n",
        "and each user can using different unicode for example in arabic we have different (i) , and 2 same word in term of meaning but in different unicode format ( font) can be completly different in computer vision so we have to preprocessing text before using in computer.\n"
      ],
      "metadata": {
        "id": "7cbRnlqRcLTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preprocessing\n",
        "- **normalizer**\n",
        "\n",
        "\n",
        "\n",
        "The goal of text normalization is to create a cleaner, more structured, and standardized representation of text that can be used effectively for various NLP tasks.\n",
        "\n",
        "\n",
        "The job of text normalization includes several tasks, which may vary depending on the specific language and requirements of the NLP (Natural Language Processing) application. Here are some common tasks performed as part of text normalization:\n",
        "\n",
        "**Lowercasing**: Converting all characters to lowercase. This ensures that words are treated as the same regardless of their capitalization.\n",
        "\n",
        "**Removing Punctuation**: Eliminating punctuation marks, symbols, and special characters that do not carry significant meaning in the analysis.\n",
        "\n",
        "**Expanding Contractions**: Replacing contractions like \"can't\" with their full forms, such as \"cannot.\"\n",
        "\n",
        "**Removing Accents/Diacritics:** Stripping accents or diacritics from characters, especially in languages with accented characters like é, è, or ü.\n",
        "\n",
        "**Handling Numbers**: Treating numbers consistently by converting them to a common format (e.g., replacing digits with a generic symbol like \"#\") or removing them entirely, depending on the analysis.\n",
        "\n",
        "**Removing Stop Words**: Eliminating common and uninformative words (stop words) like \"and,\" \"the,\" \"in,\" etc., from the text.\n",
        "\n",
        "**Stemming and Lemmatization**: Reducing words to their base or root form. Stemming and lemmatization are techniques used to reduce inflected or derived words to their dictionary form.\n",
        "\n",
        "**Handling URLs and Email Addresses**: Replacing URLs and email addresses with placeholders or removing them, as they may not provide valuable information for certain tasks.\n",
        "\n",
        "**Handling Special Tokens**: Dealing with specific tokens, such as placeholders for user mentions in social media texts (e.g., replacing \"@username\" with a special token).\n",
        "\n",
        "**Whitespace and Extra Spaces**: Removing extra whitespace, leading or trailing spaces, and ensuring consistent spacing between words.\n"
      ],
      "metadata": {
        "id": "H6n4W8PZd2Hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"بچّه ها از آموزگار خود تشکّر میکردند\""
      ],
      "metadata": {
        "id": "qc52uQ3Cfbsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize=Normalizer()\n",
        "normal_text= normalize.normalize(text)"
      ],
      "metadata": {
        "id": "j6Bp3jFdhAMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_text\n",
        "# remove shadda , consider half_space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Sq2HQnhWhTKE",
        "outputId": "7cf5c57b-749f-4b5c-e4d3-e12321ade659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'بچه\\u200cها از آموزگار خود تشکر می\\u200cکردند'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming** is a text normalization technique used in natural language processing (NLP) to reduce words to their base or root form by removing suffixes or prefixes. The resulting form, known as the \"stem,\" may not always be a valid word, but it represents the core meaning of the original word. Stemming is important in NLP for several reasons:\n",
        "\n",
        "\n",
        "**Sentiment Analysis**: In sentiment analysis, stemming can help identify the sentiment-bearing words more effectively. Stemming can group together variations of sentiment-related words, making it easier to determine the overall sentiment of a piece of text.\n",
        "\n",
        "**Text Classification**: Stemming is often used in text classification tasks to reduce the dimensionality of feature vectors. It simplifies the representation of text data while retaining essential information.\n",
        "\n",
        "**Language Processing Efficiency**: Stemming contributes to the efficiency of NLP systems. It reduces the computational resources required for text analysis, making text processing faster and more manageable.\n",
        "\n",
        "**Reducing Vocabulary Size**: Stemming helps reduce the vocabulary size in NLP tasks. By converting words with similar stems to a common form, you can reduce the number of unique words that need to be processed, which can lead to more efficient and faster text analysis.\n",
        "\n",
        "**Improved Text Matching:** Stemming allows for better matching of words with the same root. Words that share the same stem are often related in meaning. For example, \"run,\" \"running,\" and \"runner\" all have the same stem, \"run.\" Stemming helps in capturing the semantic similarity between these words.\n",
        "\n",
        "**Information Retrieval** : In information retrieval tasks like search engines, stemming helps match user queries with documents containing related words. It broadens the scope of search results by considering various word forms.\n",
        "\n",
        "**Topic Modeling and Clustering**: Stemming can be beneficial in topic modeling and text clustering tasks. It groups similar documents together by reducing words to their base forms, which can reveal underlying themes or topics.\n"
      ],
      "metadata": {
        "id": "NXFpF4VOi0eT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install NLTK and download the Porter Stemmer data (only needed once)\n",
        "#!pip install nltk\n",
        "#import nltk\n",
        "#nltk.download('punkt')\n",
        "\n",
        "# Import the Porter Stemmer from NLTK\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# Create a list of tokens\n",
        "tokens = [\"conection\", \"connected\", \"disconnected\", \"connect\", \"connecting\"]\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "# Perform stemming on the tokens and print the results\n",
        "for token in tokens:\n",
        "    stemmed_token = porter.stem(token)\n",
        "    print(token + \" --> \" + stemmed_token)\n",
        "\n",
        "# porter stemmer has \"rule tabels\" and according that rules find root."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mPp9hmMjPiZ",
        "outputId": "6fa69293-46f0-4e66-c31e-084d38e87cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conection --> conect\n",
            "connected --> connect\n",
            "disconnected --> disconnect\n",
            "connect --> connect\n",
            "connecting --> connect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Lancaster Stemmer from NLTK\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "# Create a list of tokens\n",
        "tokens = [\"lovely\", \"decentralized\", \"better\", \"information\", \"disable\", \"did\"]\n",
        "\n",
        "# Initialize the Lancaster Stemmer\n",
        "lancaster = LancasterStemmer()\n",
        "\n",
        "\n",
        "for token in tokens:\n",
        "    print(token + \" --> \" + porter.stem(token))\n",
        "    print(token + \" --> \" + lancaster.stem(token))\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "# as you can see lancster is \"over-steming\" becouase the difined algorithum iteratively do stemining till end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZTkFvB3lmM4",
        "outputId": "89707512-eb5e-4a1b-bd83-6d8b69062620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lovely --> love\n",
            "lovely --> lov\n",
            "\n",
            "\n",
            "decentralized --> decentr\n",
            "decentralized --> dec\n",
            "\n",
            "\n",
            "better --> better\n",
            "better --> bet\n",
            "\n",
            "\n",
            "information --> inform\n",
            "information --> inform\n",
            "\n",
            "\n",
            "disable --> disabl\n",
            "disable --> dis\n",
            "\n",
            "\n",
            "did --> did\n",
            "did --> did\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ", snowball\n",
        "# Snowball  is advanced porter for different languages\n",
        "# Import the Snowball Stemmer from NLTK\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Create a list of tokens\n",
        "tokens = [\"running\", \"flies\", \"happily\", \"strangely\", \"jumped\"]\n",
        "\n",
        "# Initialize the Snowball Stemmer for English\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "\n",
        "# Perform stemming on the tokens and print the results\n",
        "for token in tokens:\n",
        "    print(token + \" p--> \" + porter.stem(token))\n",
        "    print(token + \" l--> \" + lancaster.stem(token))\n",
        "    print(token + \" s--> \" + snowball.stem(token))\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43DbTzz8pMCK",
        "outputId": "4da789fc-9d8b-4387-855e-16c340d26b5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running p--> run\n",
            "running l--> run\n",
            "running s--> run\n",
            "\n",
            "\n",
            "flies p--> fli\n",
            "flies l--> fli\n",
            "flies s--> fli\n",
            "\n",
            "\n",
            "happily p--> happili\n",
            "happily l--> happy\n",
            "happily s--> happili\n",
            "\n",
            "\n",
            "strangely p--> strang\n",
            "strangely l--> strangely\n",
            "strangely s--> strang\n",
            "\n",
            "\n",
            "jumped p--> jump\n",
            "jumped l--> jump\n",
            "jumped s--> jump\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the WordNet data (only needed once)\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_JccFrLxNw1",
        "outputId": "0cb362a2-0097-4cb0-db24-8a5395cb1896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#lema is root of words that has meaning (according to dictionary )\n",
        "\n",
        "# Import the WordNet Lemmatizer from NLTK\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Create a list of tokens\n",
        "tokens = [\"lovely\", \"decentralized\", \"better\", \"information\", \"disable\", \"did\"]\n",
        "\n",
        "# Initialize the WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Perform lemmatization on the tokens and print the results\n",
        "for token in tokens:\n",
        "    print(\"stem(porter)\" + \" --> \" + porter.stem(token))\n",
        "    print(\"lemma\" + \" ---> \" + lemmatizer.lemmatize(token))\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13nP1FPNutZ7",
        "outputId": "fbf5d00e-a17e-4f21-a6e0-e849b1f90659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stem(porter) --> love\n",
            "lemma ---> lovely\n",
            "\n",
            "\n",
            "stem(porter) --> decentr\n",
            "lemma ---> decentralized\n",
            "\n",
            "\n",
            "stem(porter) --> better\n",
            "lemma ---> better\n",
            "\n",
            "\n",
            "stem(porter) --> inform\n",
            "lemma ---> information\n",
            "\n",
            "\n",
            "stem(porter) --> disabl\n",
            "lemma ---> disable\n",
            "\n",
            "\n",
            "stem(porter) --> did\n",
            "lemma ---> did\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#POS = part of speech ---> adjective, verb, none, ...\n",
        "# Perform lemmatization on the tokens and print the results\n",
        "for token in tokens:\n",
        "    print(\"stem(porter)\" + \" --> \" + porter.stem(token))\n",
        "    print(\"lemma\" + \" ---> \" + lemmatizer.lemmatize(token, pos=\"a\"))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gf7_NoOex5yC",
        "outputId": "96aca30b-75b1-4146-8e96-7fba414f77ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stem(porter) --> love\n",
            "lemma ---> lovely\n",
            "\n",
            "\n",
            "stem(porter) --> decentr\n",
            "lemma ---> decentralized\n",
            "\n",
            "\n",
            "stem(porter) --> better\n",
            "lemma ---> good\n",
            "\n",
            "\n",
            "stem(porter) --> inform\n",
            "lemma ---> information\n",
            "\n",
            "\n",
            "stem(porter) --> disabl\n",
            "lemma ---> disable\n",
            "\n",
            "\n",
            "stem(porter) --> did\n",
            "lemma ---> did\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in tokens:\n",
        "    print(\"stem(porter)\" + \" --> \" + porter.stem(token))\n",
        "    print(\"lemma\" + \" ---> \" + lemmatizer.lemmatize(token, pos=\"v\"))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvHhGDBfyY3H",
        "outputId": "5f5515d0-215b-4139-d5b1-d7ce6ea36479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stem(porter) --> love\n",
            "lemma ---> lovely\n",
            "\n",
            "\n",
            "stem(porter) --> decentr\n",
            "lemma ---> decentralize\n",
            "\n",
            "\n",
            "stem(porter) --> better\n",
            "lemma ---> better\n",
            "\n",
            "\n",
            "stem(porter) --> inform\n",
            "lemma ---> information\n",
            "\n",
            "\n",
            "stem(porter) --> disabl\n",
            "lemma ---> disable\n",
            "\n",
            "\n",
            "stem(porter) --> did\n",
            "lemma ---> do\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "in **spam detection and sentiment analysis** we dont need accuracy so stem is best choice, also **stemizer** is more faster than lemmatizer.\n",
        "\n",
        "but in **chatbot** we have to return meaningful words so we have to use **lemmatization**.\n"
      ],
      "metadata": {
        "id": "WfPvELPWy7eI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# bag of words (BOW)"
      ],
      "metadata": {
        "id": "Uj27pYr-BNGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwgmFR9SDWIP",
        "outputId": "dcf80165-4d40-4758-e81d-a053e955ff34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqLlKn5MD3Nf",
        "outputId": "8a24250b-ad97-415f-84bb-4c4293417015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "zZqzg1DwDpLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the four documents\n",
        "documents = [\n",
        "    \"the movie was terrible.\",\n",
        "    \"the movie was perfect.\",\n",
        "    \"the story of the movie was good.\",\n",
        "    \"the movie was terrible and violent.\"\n",
        "]\n",
        "\n",
        "# Tokenize the documents and convert to lowercase\n",
        "tokens = [word.lower() for doc in documents for word in nltk.word_tokenize(doc)]\n",
        "\n",
        "# Define a list of English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stop words\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "# Create a vocabulary (unique words)\n",
        "vocabulary = list(set(filtered_tokens))\n",
        "\n",
        "# Create a bag of words (term frequency) table\n",
        "bag_of_words = []\n",
        "for doc in documents:\n",
        "    word_count = Counter(nltk.word_tokenize(doc.lower()))\n",
        "    row = [word_count[word] for word in vocabulary]\n",
        "    bag_of_words.append(row)\n",
        "\n",
        "# Create a DataFrame for the bag of words\n",
        "bag_of_words_df = pd.DataFrame(bag_of_words, columns=vocabulary)\n",
        "\n",
        "# Display the results\n",
        "print(\"Vocabulary (Unique Words):\")\n",
        "print(vocabulary)\n",
        "\n",
        "print(\"\\nStop Words:\")\n",
        "print(stop_words)\n",
        "\n",
        "print(\"\\nBag of Words Table:\")\n",
        "print(bag_of_words_df)\n",
        "\n",
        "#problem of BOW: for example in sentence 1 the value of \"movie \" and \" terribe\" is the same [1] but we know the value of \"teribble\" is more than movie."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZiLUyhYDBzP",
        "outputId": "43003a6b-cef9-4849-8cac-c162b553f88f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary (Unique Words):\n",
            "['.', 'violent', 'story', 'perfect', 'good', 'terrible', 'movie']\n",
            "\n",
            "Stop Words:\n",
            "{'this', \"you'll\", 'you', 'doing', 'yours', 'which', 'is', 'your', 'down', 'wasn', 'who', 'the', 'itself', \"weren't\", 'nor', 'were', 'further', 'hers', \"mightn't\", \"you're\", 'myself', \"hadn't\", 'for', 'having', 's', 'ma', 'will', \"shouldn't\", 'then', 'my', \"won't\", 'in', 'it', 'what', 'there', 'under', 'no', 'so', 'didn', 'all', \"you've\", 'very', \"aren't\", 're', \"isn't\", 've', 'was', 'other', 'against', 'of', \"haven't\", 'own', 'mustn', 'by', 'where', 'himself', 'just', \"doesn't\", 'our', 'again', 'now', 'me', 'are', \"hasn't\", 'his', 'with', \"that'll\", 'm', 'before', 'those', 'y', 'herself', 'not', 'through', 'll', \"couldn't\", 'd', 'i', 'same', 'we', 'but', 'above', 'yourself', 'few', 'theirs', 'both', 'between', 'being', 'and', 'each', 'shan', 'weren', 'isn', 'has', 'wouldn', 'they', 'ours', 'while', 'whom', 'themselves', 'ain', 'their', 'do', 'does', \"shan't\", 'hasn', 'from', 'over', 'he', 'am', 'haven', 'ourselves', 'out', 'as', \"you'd\", 'why', 'won', 'yourselves', 'here', 'once', \"needn't\", 'did', 'aren', 'couldn', 'to', 'hadn', 'until', 't', 'should', 'at', 'him', 'she', 'o', \"didn't\", 'an', 'these', 'about', 'mightn', \"she's\", 'up', \"mustn't\", 'when', 'more', 'because', 'how', \"should've\", 'needn', 'off', 'during', 'been', 'its', 'than', 'don', 'a', 'if', \"wouldn't\", 'into', 'them', 'shouldn', 'her', 'doesn', 'have', 'such', 'any', 'on', 'or', \"don't\", 'had', 'too', \"it's\", 'only', 'some', 'be', 'can', \"wasn't\", 'after', 'below', 'most', 'that'}\n",
            "\n",
            "Bag of Words Table:\n",
            "   .  violent  story  perfect  good  terrible  movie\n",
            "0  1        0      0        0     0         1      1\n",
            "1  1        0      0        1     0         0      1\n",
            "2  1        0      1        0     1         0      1\n",
            "3  1        1      0        0     0         1      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define the four documents\n",
        "documents = [\n",
        "    \"the movie was terrible.\",\n",
        "    \"the movie was perfect.\",\n",
        "    \"the story of the movie was good.\",\n",
        "    \"the movie was terrible and violent.\"\n",
        "]\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Calculate TF-IDF values\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get feature names (terms)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create a custom function to calculate and format TF-IDF values with calculations\n",
        "def tfidf_with_calculation(tfidf_matrix, feature_names):\n",
        "    tfidf_with_calc = []\n",
        "    for row in range(tfidf_matrix.shape[0]):\n",
        "        tfidf_values = tfidf_matrix.getrow(row).toarray()[0]\n",
        "        tfidf_calculations = [f'{tfidf:.3f} * log(1 + {tfidf:.3f})' for tfidf in tfidf_values]\n",
        "        tfidf_with_calc.append(tfidf_calculations)\n",
        "    return tfidf_with_calc\n",
        "\n",
        "# Create a DataFrame to display TF-IDF calculations\n",
        "tfidf_df_with_calc = pd.DataFrame(tfidf_with_calculation(tfidf_matrix, feature_names), columns=feature_names)\n",
        "\n",
        "# Display the TF-IDF DataFrame with calculations\n",
        "print(tfidf_df_with_calc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZGfQijUGYqA",
        "outputId": "81f5b012-2c4d-4ab0-cf4f-5d8cad3097a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      and                    good                   movie  \\\n",
            "0  0.000 * log(1 + 0.000)  0.000 * log(1 + 0.000)  0.435 * log(1 + 0.435)   \n",
            "1  0.000 * log(1 + 0.000)  0.000 * log(1 + 0.000)  0.387 * log(1 + 0.387)   \n",
            "2  0.000 * log(1 + 0.000)  0.465 * log(1 + 0.465)  0.242 * log(1 + 0.242)   \n",
            "3  0.539 * log(1 + 0.539)  0.000 * log(1 + 0.000)  0.281 * log(1 + 0.281)   \n",
            "\n",
            "                       of                 perfect                   story  \\\n",
            "0  0.000 * log(1 + 0.000)  0.000 * log(1 + 0.000)  0.000 * log(1 + 0.000)   \n",
            "1  0.000 * log(1 + 0.000)  0.742 * log(1 + 0.742)  0.000 * log(1 + 0.000)   \n",
            "2  0.465 * log(1 + 0.465)  0.000 * log(1 + 0.000)  0.465 * log(1 + 0.465)   \n",
            "3  0.000 * log(1 + 0.000)  0.000 * log(1 + 0.000)  0.000 * log(1 + 0.000)   \n",
            "\n",
            "                 terrible                     the                 violent  \\\n",
            "0  0.657 * log(1 + 0.657)  0.435 * log(1 + 0.435)  0.000 * log(1 + 0.000)   \n",
            "1  0.000 * log(1 + 0.000)  0.387 * log(1 + 0.387)  0.000 * log(1 + 0.000)   \n",
            "2  0.000 * log(1 + 0.000)  0.485 * log(1 + 0.485)  0.000 * log(1 + 0.000)   \n",
            "3  0.425 * log(1 + 0.425)  0.281 * log(1 + 0.281)  0.539 * log(1 + 0.539)   \n",
            "\n",
            "                      was  \n",
            "0  0.435 * log(1 + 0.435)  \n",
            "1  0.387 * log(1 + 0.387)  \n",
            "2  0.242 * log(1 + 0.242)  \n",
            "3  0.281 * log(1 + 0.281)  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Define the four documents\n",
        "documents = [\n",
        "    \"the movie was terrible.\",\n",
        "    \"the movie was perfect.\",\n",
        "    \"the story of the movie was good.\",\n",
        "    \"the movie was terrible and violent.\"\n",
        "]\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Calculate TF-IDF values\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Create a DataFrame to display TF-IDF calculations\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Display the TF-IDF DataFrame with calculations\n",
        "print(tfidf_df.applymap(lambda x: f'{x:.3f}'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEh__SRrGYu2",
        "outputId": "b2346ffd-cb06-4ed4-ad41-363d4bd41666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     and   good  movie     of perfect  story terrible    the violent    was\n",
            "0  0.000  0.000  0.435  0.000   0.000  0.000    0.657  0.435   0.000  0.435\n",
            "1  0.000  0.000  0.387  0.000   0.742  0.000    0.000  0.387   0.000  0.387\n",
            "2  0.000  0.465  0.242  0.465   0.000  0.465    0.000  0.485   0.000  0.242\n",
            "3  0.539  0.000  0.281  0.000   0.000  0.000    0.425  0.281   0.539  0.281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Cleaning(pre-processing)\n",
        "\n",
        "- **tokenizing**\n",
        "\n",
        "- sentence tokenizing"
      ],
      "metadata": {
        "id": "dCFwoW-bvFbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Text from the previous answer\n",
        "text = \"\"\"\n",
        "Natural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language.\n",
        "The ultimate goal of NLP is to enable computers to understand, interpret, and generate human language in a valuable way.\n",
        "NLP encompasses a wide range of tasks and applications, including but not limited to:\n",
        "- Text Classification: Categorizing text documents into predefined categories or labels, such as spam detection or sentiment analysis.\n",
        "- Named Entity Recognition (NER): Identifying and classifying entities mentioned in text, such as names of people, places, and organizations.\n",
        "- Machine Translation: Translating text from one language to another, such as automatic language translation systems.\n",
        "- Question Answering: Building systems that can answer questions posed in natural language, often used in chatbots and virtual assistants.\n",
        "- Speech Recognition: Converting spoken language into text, used in voice assistants like Siri and Google Assistant.\n",
        "- Text Generation: Creating human-like text, including chat responses, content generation, and storytelling.\n",
        "In NLP, two fundamental techniques for text analysis are Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF).\n",
        "BoW represents text as a collection of words, ignoring grammar and word order, while TF-IDF assigns weights to words based on their importance in a document relative to a collection of documents.\n",
        "Both techniques are commonly used for various NLP tasks and provide valuable insights into text data.\n",
        "\"\"\"\n",
        "# first step for nlp is claening and first step of cleaning seperate all sentences in the corpos.\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# Print the extracted sentences\n",
        "sentences\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr0cIPo4zaYR",
        "outputId": "d611df55-7c10-4910-b15a-8ad99fbb6356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\nNatural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language.',\n",
              " 'The ultimate goal of NLP is to enable computers to understand, interpret, and generate human language in a valuable way.',\n",
              " 'NLP encompasses a wide range of tasks and applications, including but not limited to: \\n- Text Classification: Categorizing text documents into predefined categories or labels, such as spam detection or sentiment analysis.',\n",
              " '- Named Entity Recognition (NER): Identifying and classifying entities mentioned in text, such as names of people, places, and organizations.',\n",
              " '- Machine Translation: Translating text from one language to another, such as automatic language translation systems.',\n",
              " '- Question Answering: Building systems that can answer questions posed in natural language, often used in chatbots and virtual assistants.',\n",
              " '- Speech Recognition: Converting spoken language into text, used in voice assistants like Siri and Google Assistant.',\n",
              " '- Text Generation: Creating human-like text, including chat responses, content generation, and storytelling.',\n",
              " 'In NLP, two fundamental techniques for text analysis are Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF).',\n",
              " 'BoW represents text as a collection of words, ignoring grammar and word order, while TF-IDF assigns weights to words based on their importance in a document relative to a collection of documents.',\n",
              " 'Both techniques are commonly used for various NLP tasks and provide valuable insights into text data.']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fromsentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0FbNIDwI8wV",
        "outputId": "93e932d4-5355-4258-cc0f-f274599865bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\nNatural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language.',\n",
              " 'The ultimate goal of NLP is to enable computers to understand, interpret, and generate human language in a valuable way.',\n",
              " 'NLP encompasses a wide range of tasks and applications, including but not limited to: \\n- Text Classification: Categorizing text documents into predefined categories or labels, such as spam detection or sentiment analysis.',\n",
              " '- Named Entity Recognition (NER): Identifying and classifying entities mentioned in text, such as names of people, places, and organizations.',\n",
              " '- Machine Translation: Translating text from one language to another, such as automatic language translation systems.',\n",
              " '- Question Answering: Building systems that can answer questions posed in natural language, often used in chatbots and virtual assistants.',\n",
              " '- Speech Recognition: Converting spoken language into text, used in voice assistants like Siri and Google Assistant.',\n",
              " '- Text Generation: Creating human-like text, including chat responses, content generation, and storytelling.',\n",
              " 'In NLP, two fundamental techniques for text analysis are Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF).',\n",
              " 'BoW represents text as a collection of words, ignoring grammar and word order, while TF-IDF assigns weights to words based on their importance in a document relative to a collection of documents.',\n",
              " 'Both techniques are commonly used for various NLP tasks and provide valuable insights into text data.']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **sentence cleaning**"
      ],
      "metadata": {
        "id": "67THGWxBwFWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install regex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swwwbloayb_G",
        "outputId": "f17b792a-dddd-42d0-a6d6-eb31bc6cfd87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Text from the previous answer\n",
        "text = \"\"\"\n",
        "Natural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language.\n",
        "The ultimate goal of NLP is to enable computers to understand, interpret, and generate human language in a valuable way.\n",
        "NLP encompasses a wide range of tasks and applications, including but not limited to:\n",
        "- Text Classification: Categorizing text documents into predefined categories or labels, such as spam detection or sentiment analysis.\n",
        "- Named Entity Recognition (NER): Identifying and classifying entities mentioned in text, such as names of people, places, and organizations.\n",
        "- Machine Translation: Translating text from one language to another, such as automatic language translation systems.\n",
        "- Question Answering: Building systems that can answer questions posed in natural language, often used in chatbots and virtual assistants.\n",
        "- Speech Recognition: Converting spoken language into text, used in voice assistants like Siri and Google Assistant.\n",
        "- Text Generation: Creating human-like text, including chat responses, content generation, and storytelling.\n",
        "In NLP, two fundamental techniques for text analysis are Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF).\n",
        "BoW represents text as a collection of words, ignoring grammar and word order, while TF-IDF assigns weights to words based on their importance in a document relative to a collection of documents.\n",
        "Both techniques are commonly used for various NLP tasks and provide valuable insights into text data.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# Process and print each sentence\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    # Remove non-alphabetic characters\n",
        "    processed_sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
        "    print(f\"{i}. {processed_sentence.strip()}\")\n",
        "# as you can see after processing: all digit, signs, characters, . , .... removed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4PZ52l0j9iI",
        "outputId": "6e8bdb39-12f5-49e9-dfc6-b9a96abae527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Natural language processing  NLP  is a subfield of artificial intelligence  AI  that focuses on the interaction between computers and humans through natural language\n",
            "2. The ultimate goal of NLP is to enable computers to understand  interpret  and generate human language in a valuable way\n",
            "3. NLP encompasses a wide range of tasks and applications  including but not limited to     Text Classification  Categorizing text documents into predefined categories or labels  such as spam detection or sentiment analysis\n",
            "4. Named Entity Recognition  NER   Identifying and classifying entities mentioned in text  such as names of people  places  and organizations\n",
            "5. Machine Translation  Translating text from one language to another  such as automatic language translation systems\n",
            "6. Question Answering  Building systems that can answer questions posed in natural language  often used in chatbots and virtual assistants\n",
            "7. Speech Recognition  Converting spoken language into text  used in voice assistants like Siri and Google Assistant\n",
            "8. Text Generation  Creating human like text  including chat responses  content generation  and storytelling\n",
            "9. In NLP  two fundamental techniques for text analysis are Bag of Words  BoW  and Term Frequency Inverse Document Frequency  TF IDF\n",
            "10. BoW represents text as a collection of words  ignoring grammar and word order  while TF IDF assigns weights to words based on their importance in a document relative to a collection of documents\n",
            "11. Both techniques are commonly used for various NLP tasks and provide valuable insights into text data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lower alphabet\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    # Remove non-alphabetic characters\n",
        "    processed_sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
        "    #  Lowercase the sentence\n",
        "    processed_sentence = processed_sentence.lower()\n",
        "    print(f\"{i}. {processed_sentence.strip()}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5Lx3lSEk_Oo",
        "outputId": "717a8d7a-4300-4a42-d3eb-5844092ea618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. natural language processing  nlp  is a subfield of artificial intelligence  ai  that focuses on the interaction between computers and humans through natural language\n",
            "2. the ultimate goal of nlp is to enable computers to understand  interpret  and generate human language in a valuable way\n",
            "3. nlp encompasses a wide range of tasks and applications  including but not limited to     text classification  categorizing text documents into predefined categories or labels  such as spam detection or sentiment analysis\n",
            "4. named entity recognition  ner   identifying and classifying entities mentioned in text  such as names of people  places  and organizations\n",
            "5. machine translation  translating text from one language to another  such as automatic language translation systems\n",
            "6. question answering  building systems that can answer questions posed in natural language  often used in chatbots and virtual assistants\n",
            "7. speech recognition  converting spoken language into text  used in voice assistants like siri and google assistant\n",
            "8. text generation  creating human like text  including chat responses  content generation  and storytelling\n",
            "9. in nlp  two fundamental techniques for text analysis are bag of words  bow  and term frequency inverse document frequency  tf idf\n",
            "10. bow represents text as a collection of words  ignoring grammar and word order  while tf idf assigns weights to words based on their importance in a document relative to a collection of documents\n",
            "11. both techniques are commonly used for various nlp tasks and provide valuable insights into text data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(sentences, 1):\n",
        "    # Remove non-alphabetic characters\n",
        "    processed_sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
        "    # Lowercase the sentence\n",
        "    processed_sentence = processed_sentence.lower()\n",
        "    # Split the sentence into words\n",
        "    words = processed_sentence.split()\n",
        "\n",
        "    # Print each word in the processed sentence\n",
        "    processed_sentence = ' '.join(words)  # Join the words back into a sentence\n",
        "    print(f\"{i}. {processed_sentence.strip()}\")\n",
        "    # Print the list of words\n",
        "    print(f\"{i}. {words}\")\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0w0H2gkl_3-",
        "outputId": "61893bb4-2380-4202-cc10-f2a679960e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. natural language processing nlp is a subfield of artificial intelligence ai that focuses on the interaction between computers and humans through natural language\n",
            "1. ['natural', 'language', 'processing', 'nlp', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', 'ai', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'through', 'natural', 'language']\n",
            "\n",
            "\n",
            "2. the ultimate goal of nlp is to enable computers to understand interpret and generate human language in a valuable way\n",
            "2. ['the', 'ultimate', 'goal', 'of', 'nlp', 'is', 'to', 'enable', 'computers', 'to', 'understand', 'interpret', 'and', 'generate', 'human', 'language', 'in', 'a', 'valuable', 'way']\n",
            "\n",
            "\n",
            "3. nlp encompasses a wide range of tasks and applications including but not limited to text classification categorizing text documents into predefined categories or labels such as spam detection or sentiment analysis\n",
            "3. ['nlp', 'encompasses', 'a', 'wide', 'range', 'of', 'tasks', 'and', 'applications', 'including', 'but', 'not', 'limited', 'to', 'text', 'classification', 'categorizing', 'text', 'documents', 'into', 'predefined', 'categories', 'or', 'labels', 'such', 'as', 'spam', 'detection', 'or', 'sentiment', 'analysis']\n",
            "\n",
            "\n",
            "4. named entity recognition ner identifying and classifying entities mentioned in text such as names of people places and organizations\n",
            "4. ['named', 'entity', 'recognition', 'ner', 'identifying', 'and', 'classifying', 'entities', 'mentioned', 'in', 'text', 'such', 'as', 'names', 'of', 'people', 'places', 'and', 'organizations']\n",
            "\n",
            "\n",
            "5. machine translation translating text from one language to another such as automatic language translation systems\n",
            "5. ['machine', 'translation', 'translating', 'text', 'from', 'one', 'language', 'to', 'another', 'such', 'as', 'automatic', 'language', 'translation', 'systems']\n",
            "\n",
            "\n",
            "6. question answering building systems that can answer questions posed in natural language often used in chatbots and virtual assistants\n",
            "6. ['question', 'answering', 'building', 'systems', 'that', 'can', 'answer', 'questions', 'posed', 'in', 'natural', 'language', 'often', 'used', 'in', 'chatbots', 'and', 'virtual', 'assistants']\n",
            "\n",
            "\n",
            "7. speech recognition converting spoken language into text used in voice assistants like siri and google assistant\n",
            "7. ['speech', 'recognition', 'converting', 'spoken', 'language', 'into', 'text', 'used', 'in', 'voice', 'assistants', 'like', 'siri', 'and', 'google', 'assistant']\n",
            "\n",
            "\n",
            "8. text generation creating human like text including chat responses content generation and storytelling\n",
            "8. ['text', 'generation', 'creating', 'human', 'like', 'text', 'including', 'chat', 'responses', 'content', 'generation', 'and', 'storytelling']\n",
            "\n",
            "\n",
            "9. in nlp two fundamental techniques for text analysis are bag of words bow and term frequency inverse document frequency tf idf\n",
            "9. ['in', 'nlp', 'two', 'fundamental', 'techniques', 'for', 'text', 'analysis', 'are', 'bag', 'of', 'words', 'bow', 'and', 'term', 'frequency', 'inverse', 'document', 'frequency', 'tf', 'idf']\n",
            "\n",
            "\n",
            "10. bow represents text as a collection of words ignoring grammar and word order while tf idf assigns weights to words based on their importance in a document relative to a collection of documents\n",
            "10. ['bow', 'represents', 'text', 'as', 'a', 'collection', 'of', 'words', 'ignoring', 'grammar', 'and', 'word', 'order', 'while', 'tf', 'idf', 'assigns', 'weights', 'to', 'words', 'based', 'on', 'their', 'importance', 'in', 'a', 'document', 'relative', 'to', 'a', 'collection', 'of', 'documents']\n",
            "\n",
            "\n",
            "11. both techniques are commonly used for various nlp tasks and provide valuable insights into text data\n",
            "11. ['both', 'techniques', 'are', 'commonly', 'used', 'for', 'various', 'nlp', 'tasks', 'and', 'provide', 'valuable', 'insights', 'into', 'text', 'data']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove words with only one letter\n",
        "import re\n",
        "\n",
        "# Sample text\n",
        "text = \"This is a sample text with some words like a, I, and it.\"\n",
        "\n",
        "# Define a regular expression pattern to match words with only one letter\n",
        "pattern = r'\\b[a-zA-Z]\\b'\n",
        "\n",
        "\n",
        "# method 2: Define a regular expression pattern to match words with only one letter\n",
        "# pattern = r'\\b\\w\\b'\n",
        "\n",
        "# Use re.sub to replace one-letter words with an empty string\n",
        "filtered_text = re.sub(pattern, '', text)\n",
        "\n",
        "# Print the filtered text\n",
        "print(filtered_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q550lxJxtEBe",
        "outputId": "22241c65-7288-4282-b1df-d86d50ddfc62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is  sample text with some words like , , and it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**- Word Tokenizing**"
      ],
      "metadata": {
        "id": "2-tLI75vwvpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Text from the previous answer\n",
        "text = \"\"\"\n",
        "Natural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language.\n",
        "The ultimate goal of NLP is to enable computers to understand, interpret, and generate human language in a valuable way.\n",
        "NLP encompasses a wide range of tasks and applications, including but not limited to:\n",
        "- Text Classification: Categorizing text documents into predefined categories or labels, such as spam detection or sentiment analysis.\n",
        "- Named Entity Recognition (NER): Identifying and classifying entities mentioned in text, such as names of people, places, and organizations.\n",
        "- Machine Translation: Translating text from one language to another, such as automatic language translation systems.\n",
        "- Question Answering: Building systems that can answer questions posed in natural language, often used in chatbots and virtual assistants.\n",
        "- Speech Recognition: Converting spoken language into text, used in voice assistants like Siri and Google Assistant.\n",
        "- Text Generation: Creating human-like text, including chat responses, content generation, and storytelling.\n",
        "In NLP, two fundamental techniques for text analysis are Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF).\n",
        "BoW represents text as a collection of words, ignoring grammar and word order, while TF-IDF assigns weights to words based on their importance in a document relative to a collection of documents.\n",
        "Both techniques are commonly used for various NLP tasks and provide valuable insights into text data.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# Initialize the NLTK stopwords and PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Process and print each sentence\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    # Remove non-alphabetic characters\n",
        "    processed_sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
        "    # Lowercase the sentence\n",
        "    processed_sentence = processed_sentence.lower()\n",
        "    # Tokenize the sentence into words\n",
        "    words = word_tokenize(processed_sentence)\n",
        "    # Stem the words and remove stop words\n",
        "    stemmed_words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "\n",
        "    # Print the list of stemmed words\n",
        "    print(f\"{i}. {stemmed_words}\")\n",
        "    # Print the list of stopwords\n",
        "    print(f\"   Stopwords: {set(words).intersection(stop_words)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_qt2oE2mS_9",
        "outputId": "41ff0023-d37f-481b-9174-9320ce5604d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ['natur', 'languag', 'process', 'nlp', 'subfield', 'artifici', 'intellig', 'ai', 'focus', 'interact', 'comput', 'human', 'natur', 'languag']\n",
            "   Stopwords: {'between', 'and', 'the', 'is', 'on', 'a', 'through', 'that', 'of'}\n",
            "2. ['ultim', 'goal', 'nlp', 'enabl', 'comput', 'understand', 'interpret', 'gener', 'human', 'languag', 'valuabl', 'way']\n",
            "   Stopwords: {'in', 'and', 'the', 'is', 'to', 'a', 'of'}\n",
            "3. ['nlp', 'encompass', 'wide', 'rang', 'task', 'applic', 'includ', 'limit', 'text', 'classif', 'categor', 'text', 'document', 'predefin', 'categori', 'label', 'spam', 'detect', 'sentiment', 'analysi']\n",
            "   Stopwords: {'and', 'into', 'such', 'to', 'not', 'a', 'but', 'or', 'as', 'of'}\n",
            "4. ['name', 'entiti', 'recognit', 'ner', 'identifi', 'classifi', 'entiti', 'mention', 'text', 'name', 'peopl', 'place', 'organ']\n",
            "   Stopwords: {'in', 'and', 'such', 'as', 'of'}\n",
            "5. ['machin', 'translat', 'translat', 'text', 'one', 'languag', 'anoth', 'automat', 'languag', 'translat', 'system']\n",
            "   Stopwords: {'to', 'such', 'as', 'from'}\n",
            "6. ['question', 'answer', 'build', 'system', 'answer', 'question', 'pose', 'natur', 'languag', 'often', 'use', 'chatbot', 'virtual', 'assist']\n",
            "   Stopwords: {'in', 'and', 'that', 'can'}\n",
            "7. ['speech', 'recognit', 'convert', 'spoken', 'languag', 'text', 'use', 'voic', 'assist', 'like', 'siri', 'googl', 'assist']\n",
            "   Stopwords: {'in', 'and', 'into'}\n",
            "8. ['text', 'gener', 'creat', 'human', 'like', 'text', 'includ', 'chat', 'respons', 'content', 'gener', 'storytel']\n",
            "   Stopwords: {'and'}\n",
            "9. ['nlp', 'two', 'fundament', 'techniqu', 'text', 'analysi', 'bag', 'word', 'bow', 'term', 'frequenc', 'invers', 'document', 'frequenc', 'tf', 'idf']\n",
            "   Stopwords: {'in', 'and', 'for', 'are', 'of'}\n",
            "10. ['bow', 'repres', 'text', 'collect', 'word', 'ignor', 'grammar', 'word', 'order', 'tf', 'idf', 'assign', 'weight', 'word', 'base', 'import', 'document', 'rel', 'collect', 'document']\n",
            "   Stopwords: {'in', 'and', 'to', 'while', 'on', 'a', 'their', 'as', 'of'}\n",
            "11. ['techniqu', 'commonli', 'use', 'variou', 'nlp', 'task', 'provid', 'valuabl', 'insight', 'text', 'data']\n",
            "   Stopwords: {'and', 'into', 'for', 'are', 'both'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**- last step make CorPus**"
      ],
      "metadata": {
        "id": "nsnK8GKHw1ER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Text from the previous answer\n",
        "text = \"\"\"\n",
        "Natural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language.\n",
        "The ultimate goal of NLP is to enable computers to understand, interpret, and generate human language in a valuable way.\n",
        "NLP encompasses a wide range of tasks and applications, including but not limited to:\n",
        "- Text Classification: Categorizing text documents into predefined categories or labels, such as spam detection or sentiment analysis.\n",
        "- Named Entity Recognition (NER): Identifying and classifying entities mentioned in text, such as names of people, places, and organizations.\n",
        "- Machine Translation: Translating text from one language to another, such as automatic language translation systems.\n",
        "- Question Answering: Building systems that can answer questions posed in natural language, often used in chatbots and virtual assistants.\n",
        "- Speech Recognition: Converting spoken language into text, used in voice assistants like Siri and Google Assistant.\n",
        "- Text Generation: Creating human-like text, including chat responses, content generation, and storytelling.\n",
        "In NLP, two fundamental techniques for text analysis are Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF).\n",
        "BoW represents text as a collection of words, ignoring grammar and word order, while TF-IDF assigns weights to words based on their importance in a document relative to a collection of documents.\n",
        "Both techniques are commonly used for various NLP tasks and provide valuable insights into text data.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# Initialize the NLTK stopwords and PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Create a list to store processed sentences\n",
        "processed_sentences = []\n",
        "\n",
        "# Process and append each sentence to the list\n",
        "for sentence in sentences:\n",
        "    # Remove non-alphabetic characters\n",
        "    processed_sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
        "    # Lowercase the sentence\n",
        "    processed_sentence = processed_sentence.lower()\n",
        "    # Tokenize the sentence into words\n",
        "    words = word_tokenize(processed_sentence)\n",
        "    # Stem the words and remove stop words\n",
        "    stemmed_words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "    # Join the stemmed words into a sentence\n",
        "    processed_sentence = ' '.join(stemmed_words)\n",
        "    # Append the processed sentence to the list\n",
        "    processed_sentences.append(processed_sentence)\n",
        "\n",
        "# Join the processed sentences into a single corpus string\n",
        "corpus = ' '.join(processed_sentences)\n",
        "\n",
        "# Print the corpus\n",
        "print(corpus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuW3fFLPrpRl",
        "outputId": "56228a3f-f3a5-4888-b40b-5e54a1471ead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "natur languag process nlp subfield artifici intellig ai focus interact comput human natur languag ultim goal nlp enabl comput understand interpret gener human languag valuabl way nlp encompass wide rang task applic includ limit text classif categor text document predefin categori label spam detect sentiment analysi name entiti recognit ner identifi classifi entiti mention text name peopl place organ machin translat translat text one languag anoth automat languag translat system question answer build system answer question pose natur languag often use chatbot virtual assist speech recognit convert spoken languag text use voic assist like siri googl assist text gener creat human like text includ chat respons content gener storytel nlp two fundament techniqu text analysi bag word bow term frequenc invers document frequenc tf idf bow repres text collect word ignor grammar word order tf idf assign weight word base import document rel collect document techniqu commonli use variou nlp task provid valuabl insight text data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`- Vectorization`**"
      ],
      "metadata": {
        "id": "UI7_mrmCxHy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample corpus (replace with your processed corpus)\n",
        "corpus = \"\"\"\n",
        "Natural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language.\n",
        "The ultimate goal of NLP is to enable computers to understand, interpret, and generate human language in a valuable way.\n",
        "NLP encompasses a wide range of tasks and applications, including but not limited to:\n",
        "- Text Classification: Categorizing text documents into predefined categories or labels, such as spam detection or sentiment analysis.\n",
        "- Named Entity Recognition (NER): Identifying and classifying entities mentioned in text, such as names of people, places, and organizations.\n",
        "- Machine Translation: Translating text from one language to another, such as automatic language translation systems.\n",
        "- Question Answering: Building systems that can answer questions posed in natural language, often used in chatbots and virtual assistants.\n",
        "- Speech Recognition: Converting spoken language into text, used in voice assistants like Siri and Google Assistant.\n",
        "- Text Generation: Creating human-like text, including chat responses, content generation, and storytelling.\n",
        "In NLP, two fundamental techniques for text analysis are Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF).\n",
        "BoW represents text as a collection of words, ignoring grammar and word order, while TF-IDF assigns weights to words based on their importance in a document relative to a collection of documents.\n",
        "Both techniques are commonly used for various NLP tasks and provide valuable insights into text data.\n",
        "\"\"\"\n",
        "\n",
        "# Create a CountVectorizer instance\n",
        "cv = CountVectorizer()\n",
        "\n",
        "# Fit and transform the corpus to create the BoW table\n",
        "bow = cv.fit_transform([corpus])\n",
        "\n",
        "# Convert the BoW table to an array\n",
        "bow_array = bow.toarray()\n",
        "\n",
        "# Print the BoW table\n",
        "print(bow_array)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d39nYvToxQmI",
        "outputId": "9f386ed8-7c23-4d4d-c6b7-6a2dc64aad4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  2 11  1  1  1  1  2  1  4  1  1  2  1  1  1  1  1  2  1  1  1  1  1\n",
            "   1  1  1  1  2  1  2  1  1  1  1  1  2  2  1  1  1  1  1  2  2  1  1  1\n",
            "   2  1  1  1  2  1  1  2  1  1  7  2  1  1  1  1  3  1  2  1  7  2  1  1\n",
            "   1  1  1  3  1  5  1  7  1  2  1  2  1  1  1  1  1  1  1  1  1  1  1  2\n",
            "   1  1  1  1  1  1  1  1  1  1  3  2  2  2  1 10  2  2  2  1  1  6  1  2\n",
            "   1  1  1  3  2  1  1  1  1  1  1  1  1  3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample corpus (replace with your processed corpus)\n",
        "corpus = [\n",
        "    \"this is a sample sentence\",\n",
        "    \"another sample sentence\",\n",
        "    \"one more example\",\n",
        "]\n",
        "\n",
        "# Create a CountVectorizer instance\n",
        "cv = CountVectorizer()\n",
        "\n",
        "# Fit and transform the corpus to create the BoW table\n",
        "bow = cv.fit_transform(corpus)\n",
        "\n",
        "# Convert the BoW table to an array\n",
        "bow_array = bow.toarray()\n",
        "\n",
        "# Print the BoW table\n",
        "print(bow_array)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJBat-btz7_E",
        "outputId": "4dce7352-f6c2-4a53-e81d-b58ff1407157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1 0 0 1 1 1]\n",
            " [1 0 0 0 0 1 1 0]\n",
            " [0 1 0 1 1 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample corpus (replace with your processed corpus)\n",
        "corpus = \"\"\"\n",
        "Natural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language.\n",
        "The ultimate goal of NLP is to enable computers to understand, interpret, and generate human language in a valuable way.\n",
        "NLP encompasses a wide range of tasks and applications, including but not limited to:\n",
        "- Text Classification: Categorizing text documents into predefined categories or labels, such as spam detection or sentiment analysis.\n",
        "- Named Entity Recognition (NER): Identifying and classifying entities mentioned in text, such as names of people, places, and organizations.\n",
        "- Machine Translation: Translating text from one language to another, such as automatic language translation systems.\n",
        "- Question Answering: Building systems that can answer questions posed in natural language, often used in chatbots and virtual assistants.\n",
        "- Speech Recognition: Converting spoken language into text, used in voice assistants like Siri and Google Assistant.\n",
        "- Text Generation: Creating human-like text, including chat responses, content generation, and storytelling.\n",
        "In NLP, two fundamental techniques for text analysis are Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF).\n",
        "BoW represents text as a collection of words, ignoring grammar and word order, while TF-IDF assigns weights to words based on their importance in a document relative to a collection of documents.\n",
        "Both techniques are commonly used for various NLP tasks and provide valuable insights into text data.\n",
        "\"\"\"\n",
        "\n",
        "# Create a TfidfVectorizer instance\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the corpus to create the TF-IDF matrix\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform([corpus])\n",
        "\n",
        "# Convert the TF-IDF matrix to an array\n",
        "tfidf_array = tfidf_matrix.toarray()\n",
        "\n",
        "# Print the shape of the TF-IDF matrix\n",
        "print(\"Shape of TF-IDF Matrix:\", tfidf_array.shape)\n",
        "\n",
        "# Print the complete TF-IDF matrix\n",
        "print(\"Complete TF-IDF Matrix:\\n\", tfidf_array)\n",
        "\n",
        "# the form of real matrix is 134*134 matrix each word is a feature so 134 is number of words in entire corpus.\n",
        "#the TF/IDF and BOW are very sparse matrix just we have value in sentence with that feature\n",
        "# we have 134 features but each document/sentences has subset of this 134\n",
        "# sparse matrix ---> need more memory, week calculation ---> encrease complexity and decrease accuracy.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRqdekov2-RE",
        "outputId": "963e1e26-c6cd-4f5c-b793-f0a1347df890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of TF-IDF Matrix: (1, 134)\n",
            "Complete TF-IDF Matrix:\n",
            " [[0.0380143  0.07602859 0.41815726 0.0380143  0.0380143  0.0380143\n",
            "  0.0380143  0.07602859 0.0380143  0.15205718 0.0380143  0.0380143\n",
            "  0.07602859 0.0380143  0.0380143  0.0380143  0.0380143  0.0380143\n",
            "  0.07602859 0.0380143  0.0380143  0.0380143  0.0380143  0.0380143\n",
            "  0.0380143  0.0380143  0.0380143  0.0380143  0.07602859 0.0380143\n",
            "  0.07602859 0.0380143  0.0380143  0.0380143  0.0380143  0.0380143\n",
            "  0.07602859 0.07602859 0.0380143  0.0380143  0.0380143  0.0380143\n",
            "  0.0380143  0.07602859 0.07602859 0.0380143  0.0380143  0.0380143\n",
            "  0.07602859 0.0380143  0.0380143  0.0380143  0.07602859 0.0380143\n",
            "  0.0380143  0.07602859 0.0380143  0.0380143  0.26610007 0.07602859\n",
            "  0.0380143  0.0380143  0.0380143  0.0380143  0.11404289 0.0380143\n",
            "  0.07602859 0.0380143  0.26610007 0.07602859 0.0380143  0.0380143\n",
            "  0.0380143  0.0380143  0.0380143  0.11404289 0.0380143  0.19007148\n",
            "  0.0380143  0.26610007 0.0380143  0.07602859 0.0380143  0.07602859\n",
            "  0.0380143  0.0380143  0.0380143  0.0380143  0.0380143  0.0380143\n",
            "  0.0380143  0.0380143  0.0380143  0.0380143  0.0380143  0.07602859\n",
            "  0.0380143  0.0380143  0.0380143  0.0380143  0.0380143  0.0380143\n",
            "  0.0380143  0.0380143  0.0380143  0.0380143  0.11404289 0.07602859\n",
            "  0.07602859 0.07602859 0.0380143  0.38014296 0.07602859 0.07602859\n",
            "  0.07602859 0.0380143  0.0380143  0.22808578 0.0380143  0.07602859\n",
            "  0.0380143  0.0380143  0.0380143  0.11404289 0.07602859 0.0380143\n",
            "  0.0380143  0.0380143  0.0380143  0.0380143  0.0380143  0.0380143\n",
            "  0.0380143  0.11404289]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jxOoz_k53W5P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}